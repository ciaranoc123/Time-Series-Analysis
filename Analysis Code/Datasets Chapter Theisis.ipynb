{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10b8f0de",
   "metadata": {},
   "source": [
    "Stationarity Test:\n",
    "\n",
    "Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test\n",
    "\n",
    "Augmented Dickey-Fuller (ADF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e686174e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb23e98e",
   "metadata": {},
   "source": [
    "ADF for the DAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff863fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import datetime as dt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Load the dataset with error handling\n",
    "try:\n",
    "    date_format = \"%d/%m/%Y %H:%M\"\n",
    "    date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "    dat = pd.read_csv(\"/home/ciaran/Documents/DAM_VAR_1-3.csv\", index_col=\"DeliveryPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "    dat = dat._get_numeric_data()  # Extract numeric columns only\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "\n",
    "# Select the relevant time series for analysis\n",
    "Y = dat.iloc[:, 0:1]  # Assuming the first column is EURPrices; adjust if needed\n",
    "\n",
    "# Function to perform the ADF test\n",
    "def adf_test(series):\n",
    "    result = adfuller(series.dropna())  # Drop NaNs to avoid errors\n",
    "    adf_statistic = result[0]\n",
    "    p_value = result[1]\n",
    "    critical_values = result[4]\n",
    "    \n",
    "    print(f'ADF Statistic: {adf_statistic}')\n",
    "    print(f'p-value: {p_value}')\n",
    "    for key, value in critical_values.items():\n",
    "        print(f'Critical Values {key}: {value}')\n",
    "    \n",
    "    # Return results for potential further use\n",
    "    return adf_statistic, p_value, critical_values\n",
    "\n",
    "# Apply ADF test to the EURPrices series\n",
    "print(\"Initial ADF Test Results:\")\n",
    "adf_statistic, p_value, critical_values = adf_test(Y['EURPrices'])\n",
    "\n",
    "# Check stationarity and apply differencing if necessary\n",
    "if p_value > 0.05:\n",
    "    print(\"\\nSeries is non-stationary, applying differencing...\\n\")\n",
    "    Y_diff = Y.diff().dropna()\n",
    "    \n",
    "    # Reapply the ADF test to the differenced series\n",
    "    print(\"ADF Test Results after Differencing:\")\n",
    "    adf_test(Y_diff['EURPrices'])\n",
    "else:\n",
    "    print(\"\\nSeries is stationary, no differencing needed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c96dcc",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797c1c5f",
   "metadata": {},
   "source": [
    "ADF for the BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b046e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import datetime as dt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Define date format for parsing\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "\n",
    "# Function to parse dates correctly\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "\n",
    "# Load the CSV, parsing dates in the correct format and setting the index\n",
    "try:\n",
    "    dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "\n",
    "# Drop unnecessary columns and handle missing data\n",
    "dat = dat.drop([\"index\"], axis=1, errors='ignore')  # Drop the 'index' column, if it exists\n",
    "dat = dat.bfill(axis='rows')  # Backward fill missing values\n",
    "dat = dat.ffill(axis='rows')  # Forward fill missing values\n",
    "dat = dat._get_numeric_data()  # Extract numeric data only\n",
    "\n",
    "# Select the relevant time series for analysis\n",
    "Y = dat.iloc[:, 0:1]  # Selecting the first column; adjust as necessary\n",
    "\n",
    "# Perform ADF test function\n",
    "def adf_test(series):\n",
    "    result = adfuller(series.dropna())  # Drop NaNs to avoid errors\n",
    "    adf_statistic = result[0]\n",
    "    p_value = result[1]\n",
    "    critical_values = result[4]\n",
    "    \n",
    "    print(f'ADF Statistic: {adf_statistic}')\n",
    "    print(f'p-value: {p_value}')\n",
    "    for key, value in critical_values.items():\n",
    "        print(f'Critical Values {key}: {value}')\n",
    "    \n",
    "    # Return the results for potential further use\n",
    "    return adf_statistic, p_value, critical_values\n",
    "\n",
    "# Apply the ADF test to the target series\n",
    "print(\"Initial ADF Test Results:\")\n",
    "adf_statistic, p_value, critical_values = adf_test(Y['lag_2y'])\n",
    "\n",
    "# Assess stationarity based on p-value and decide if differencing is needed\n",
    "if p_value > 0.05:\n",
    "    print(\"\\nSeries is non-stationary, applying differencing...\\n\")\n",
    "    Y_diff = Y.diff().dropna()  # Apply differencing if needed\n",
    "    \n",
    "    # Reapply the ADF test to the differenced series\n",
    "    print(\"ADF Test Results after Differencing:\")\n",
    "    adf_test(Y_diff['lag_2y'])\n",
    "else:\n",
    "    print(\"\\nSeries is stationary, no differencing needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c1942",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925ed4fb",
   "metadata": {},
   "source": [
    "KPSS for the DAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b5d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import datetime as dt\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "\n",
    "# Load the dataset\n",
    "date_format = \"%d/%m/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/DAM_VAR_1-3.csv\", index_col=\"DeliveryPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "# Keep only numeric data from the dataset\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "# Select the relevant time series for analysis (e.g., EURPrices column)\n",
    "Y = dat.iloc[:, 0:1]  # Assuming EURPrices is the first column\n",
    "\n",
    "# Define a function to perform the KPSS test\n",
    "def kpss_test(series, regression='c'):\n",
    "    \"\"\"\n",
    "    Perform KPSS test on a given time series.\n",
    "    \n",
    "    Parameters:\n",
    "    - series: time series to test\n",
    "    - regression: 'c' for constant (level stationarity), 'ct' for constant + trend (trend stationarity)\n",
    "    \"\"\"\n",
    "    result = kpss(series.dropna(), regression=regression)\n",
    "    print(f'KPSS Statistic: {result[0]}')\n",
    "    print(f'p-value: {result[1]}')\n",
    "    for key, value in result[3].items():\n",
    "        print(f'Critical Values {key}: {value}')\n",
    "\n",
    "    # Return the p-value to determine if differencing is required\n",
    "    return result[1]\n",
    "\n",
    "# Apply KPSS test to check for level stationarity (regression='c')\n",
    "print(\"KPSS Test for Level Stationarity:\")\n",
    "p_value = kpss_test(Y['EURPrices'], regression='c')\n",
    "\n",
    "# If the series is not stationary (p-value < 0.05), apply differencing\n",
    "if p_value < 0.05:\n",
    "    print(\"\\nSeries is not stationary. Applying differencing...\")\n",
    "    Y_diff = Y.diff().dropna()\n",
    "    \n",
    "    # Reapply the KPSS test to the differenced data\n",
    "    print(\"KPSS Test on Differenced Data:\")\n",
    "    kpss_test(Y_diff['EURPrices'], regression='c')\n",
    "else:\n",
    "    print(\"\\nSeries is stationary. No differencing required.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62527df",
   "metadata": {},
   "source": [
    "KPSS test for the BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34643ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import datetime as dt\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "\n",
    "# Define date format for parsing\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "\n",
    "# Function to parse dates\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "\n",
    "# Read the CSV file and parse dates\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "# Remove any non-numeric columns if necessary (like 'index')\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "\n",
    "# Convert to DataFrame and handle missing values with forward and backward fill\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "\n",
    "# Keep only numeric data\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "# Select the target time series for analysis (lag_2y)\n",
    "Y = dat[['lag_2y']]\n",
    "\n",
    "# Define a function to perform the KPSS test\n",
    "def kpss_test(series, regression='c'):\n",
    "    \"\"\"\n",
    "    Perform KPSS test on a given time series.\n",
    "    \n",
    "    Parameters:\n",
    "    - series: time series to test\n",
    "    - regression: 'c' for constant (level stationarity), 'ct' for constant + trend (trend stationarity)\n",
    "    \"\"\"\n",
    "    result = kpss(series.dropna(), regression=regression)\n",
    "    print(f'KPSS Statistic: {result[0]}')\n",
    "    print(f'p-value: {result[1]}')\n",
    "    for key, value in result[3].items():\n",
    "        print(f'Critical Values {key}: {value}')\n",
    "\n",
    "    # Return the p-value to determine if differencing is required\n",
    "    return result[1]\n",
    "\n",
    "# Apply KPSS test to check for level stationarity (regression='c')\n",
    "print(\"KPSS Test for Level Stationarity (lag_2y):\")\n",
    "p_value = kpss_test(Y['lag_2y'], regression='c')\n",
    "\n",
    "# If the series is not stationary (p-value < 0.05), apply differencing\n",
    "if p_value < 0.05:\n",
    "    print(\"\\nSeries is not stationary. Applying differencing...\")\n",
    "    Y_diff = Y.diff().dropna()\n",
    "    \n",
    "    # Reapply the KPSS test to the differenced data\n",
    "    print(\"KPSS Test on Differenced Data (lag_2y):\")\n",
    "    kpss_test(Y_diff['lag_2y'], regression='c')\n",
    "else:\n",
    "    print(\"\\nSeries is stationary. No differencing required.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925ef1df",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd00a9e9",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705868da",
   "metadata": {},
   "source": [
    "Autocorrelation and Partial Autocorrelation Analysis:\n",
    "\n",
    "ACF and PACF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1412b87",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6693d847",
   "metadata": {},
   "source": [
    "ACF for the DAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e87e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import datetime as dt\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "date_format = \"%d/%m/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/DAM_VAR_1-3.csv\", index_col=\"DeliveryPeriod\", parse_dates=True, date_format=date_format)\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "# Step 2: Extract the time series data for DAM prices and other covariates\n",
    "Y_dam = dat['EURPrices']\n",
    "covariates = ['DF', 'WF']\n",
    "\n",
    "# Step 3: Plot ACF for the DAM data and covariates\n",
    "def plot_acf(series, lags=167, ax=None):\n",
    "    \"\"\"\n",
    "    Plot the Autocorrelation Function (ACF) for a given time series.\n",
    "\n",
    "    Parameters:\n",
    "    - series: The time series data to plot.\n",
    "    - lags: Number of lags to include in the plot.\n",
    "    - ax: Matplotlib axis object for plotting.\n",
    "    \"\"\"\n",
    "    sm.graphics.tsa.plot_acf(series, lags=lags, ax=ax, color='blue')\n",
    "    ax.set_xlabel('Lag', fontsize=12)\n",
    "    ax.set_ylabel('ACF', fontsize=12)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax.set_title(f'ACF - {series.name}', fontsize=14, fontweight='bold')\n",
    "\n",
    "\n",
    "# Define figure and axes\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "fig.suptitle('Autocorrelation Function Analysis for the Day-Ahead Market Dataset', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot ACF for EURPrices\n",
    "plot_acf(Y_dam, lags=167, ax=axes[0])\n",
    "axes[0].set_title('Day-Ahead Market Prices', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot ACF for DF\n",
    "if 'DF' in dat.columns:\n",
    "    plot_acf(dat['DF'], lags=167, ax=axes[1])\n",
    "    axes[1].set_title('Demand Forecast', fontsize=14, fontweight='bold')\n",
    "else:\n",
    "    axes[1].set_visible(False)  # Hide subplot if 'DF' column is missing\n",
    "\n",
    "# Plot ACF for WF\n",
    "if 'WF' in dat.columns:\n",
    "    plot_acf(dat['WF'], lags=167, ax=axes[2])\n",
    "    axes[2].set_title('Wind Forecast', fontsize=14, fontweight='bold')\n",
    "else:\n",
    "    axes[2].set_visible(False)  # Hide subplot if 'WF' column is missing\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig('/home/ciaran/ACF_DAM.jpg', format='jpg', dpi=300)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e2fa9f",
   "metadata": {},
   "source": [
    "ACF for the BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13023b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import datetime as dt\n",
    "\n",
    "# Define date format for parsing\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "\n",
    "# Function to parse dates\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "\n",
    "# Read CSV without frequency information and parse dates\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "# Drop non-numeric columns and handle missing data\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = dat.bfill(axis='rows').ffill(axis='rows')._get_numeric_data()\n",
    "\n",
    "# Select the relevant time series for analysis\n",
    "Y = dat.iloc[:, 0:1]  # EURPrices column\n",
    "X = dat.iloc[:, 16:]  # Other variables\n",
    "X = pd.concat([\n",
    "    X.iloc[:, :1],     # lag_-3x1\n",
    "    X.iloc[:, 49:50],  # lag_-3x2\n",
    "    X.iloc[:, 98:99],  # lag_-2x3\n",
    "    X.iloc[:, 147:148],# lag_0x4\n",
    "    X.iloc[:, 148:149],# lag_0x5\n",
    "    X.iloc[:, 149:150],# lag_0x6\n",
    "    X.iloc[:, 197:198],# lag_2x7\n",
    "    X.iloc[:, 213:214],# lag_2x8\n",
    "    X.iloc[:, 229:230],# lag_2x9\n",
    "    X.iloc[:, 245:246],# lag_2x10\n",
    "    X.iloc[:, 261:262],# lag_2x11\n",
    "    X.iloc[:, 277:278] # lag_-2x12\n",
    "], axis=1)\n",
    "\n",
    "# Extract specific series for historical and future-looking data\n",
    "historical_data = {\n",
    "    'BMP': X['lag_-3x1'],\n",
    "    'BMV': X['lag_-3x2'],\n",
    "    'WDiff': X['lag_-2x3'],\n",
    "    'I': X['lag_-2x12'],\n",
    "    'DAM': X['lag_0x6']\n",
    "}\n",
    "\n",
    "future_data = {\n",
    "    'PHPN': X['lag_2x7'],\n",
    "    'PHI': X['lag_2x8'],\n",
    "    'PHFW': X['lag_2x9'],\n",
    "    'PHFD': X['lag_2x10'],\n",
    "    'DAM': X['lag_2x11']\n",
    "}\n",
    "\n",
    "# Function to plot ACF\n",
    "def plot_acf(series, lags=48, ax=None):\n",
    "    \"\"\"\n",
    "    Plot the Autocorrelation Function (ACF) for a given time series.\n",
    "\n",
    "    Parameters:\n",
    "    - series: The time series data to plot.\n",
    "    - lags: Number of lags to include in the plot.\n",
    "    - ax: Matplotlib axis object for plotting.\n",
    "    \"\"\"\n",
    "    sm.graphics.tsa.plot_acf(series, lags=lags, ax=ax, color='blue')\n",
    "    ax.set_xlabel('Lag', fontsize=12)\n",
    "    ax.set_ylabel('ACF', fontsize=12)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax.set_title(f'ACF - {series.name}', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Create subplots for historical and future-looking data\n",
    "fig, axes = plt.subplots(2, 4, figsize=(36, 12), sharey=True)\n",
    "fig.suptitle('Autocorrelation Function Analysis for Balancing Market Dataset', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot ACF for historical data (first row)\n",
    "plot_acf(historical_data['BMP'], lags=48, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('BM Price', fontsize=14, fontweight='bold')\n",
    "\n",
    "plot_acf(historical_data['BMV'], lags=48, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('BM Volume', fontsize=14, fontweight='bold')\n",
    "\n",
    "plot_acf(historical_data['WDiff'], lags=48, ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Forecast Wind - Actual Wind:', fontsize=16, fontweight='bold')\n",
    "\n",
    "plot_acf(historical_data['I'], lags=48, ax=axes[0, 3])\n",
    "axes[0, 3].set_title('Interconnector Values', fontsize=14, fontweight='bold')\n",
    "\n",
    "# plot_acf(historical_data['DAM'], lags=48, ax=axes[0, 4])\n",
    "# axes[0, 4].set_title('Past DAM Prices', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot ACF for future-looking data (second row)\n",
    "plot_acf(future_data['PHPN'], lags=48, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Physical Notifications Volume', fontsize=14, fontweight='bold')\n",
    "\n",
    "plot_acf(future_data['PHI'], lags=48, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Net Interconnector Schedule', fontsize=14, fontweight='bold')\n",
    "\n",
    "plot_acf(future_data['PHFW'], lags=48, ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Renewable Forecast', fontsize=14, fontweight='bold')\n",
    "\n",
    "plot_acf(future_data['PHFD'], lags=48, ax=axes[1, 3])\n",
    "axes[1, 3].set_title('Demand Forecast', fontsize=14, fontweight='bold')\n",
    "\n",
    "# DAM is included twice in both historical and future data; use a distinct title for clarity\n",
    "# plot_acf(future_data['DAM'], lags=48, ax=axes[1, 4])\n",
    "# axes[1, 4].set_title('Future DAM Prices', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Adjust layout and save the figure\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "# Save the figure as a JPG file\n",
    "plt.savefig('/home/ciaran/BM_ACF.jpg', format='jpg', dpi=500)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ade929",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e77e26",
   "metadata": {},
   "source": [
    "Partial Autocorrelation Function for the DAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a6e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import datetime as dt\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "date_format = \"%d/%m/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/DAM_VAR_1-3.csv\", index_col=\"DeliveryPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "# Step 2: Extract the time series data for DAM prices and other covariates\n",
    "Y_dam = dat['EURPrices']\n",
    "covariates = ['DF', 'WF']\n",
    "\n",
    "# Step 3: Plot PACF for the DAM data and covariates\n",
    "def plot_pacf(series, lags=48, ax=None):\n",
    "    \"\"\"\n",
    "    Plot the Partial Autocorrelation Function (PACF) for a given time series.\n",
    "\n",
    "    Parameters:\n",
    "    - series: The time series data to plot.\n",
    "    - lags: Number of lags to include in the plot.\n",
    "    - ax: Matplotlib axis object for plotting.\n",
    "    \"\"\"\n",
    "    sm.graphics.tsa.plot_pacf(series, lags=lags, ax=ax, method='ywm', color='blue')\n",
    "    ax.set_xlabel('Lag', fontsize=12)\n",
    "    ax.set_ylabel('PACF', fontsize=12)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax.set_title(f'PACF - {series.name}', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Define figure and axes\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "fig.suptitle('Partial Autocorrelation Function Analysis for the Day-Ahead Market Dataset', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot PACF for EURPrices\n",
    "plot_pacf(Y_dam, lags=167, ax=axes[0])\n",
    "axes[0].set_title('Day-Ahead Market Prices', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot PACF for DF\n",
    "if 'DF' in dat.columns:\n",
    "    plot_pacf(dat['DF'], lags=167, ax=axes[1])\n",
    "    axes[1].set_title('Demand Forecast', fontsize=14, fontweight='bold')\n",
    "else:\n",
    "    axes[1].set_visible(False)  # Hide subplot if 'DF' column is missing\n",
    "\n",
    "# Plot PACF for WF\n",
    "if 'WF' in dat.columns:\n",
    "    plot_pacf(dat['WF'], lags=167, ax=axes[2])\n",
    "    axes[2].set_title('Wind Forecast', fontsize=14, fontweight='bold')\n",
    "else:\n",
    "    axes[2].set_visible(False)  # Hide subplot if 'WF' column is missing\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig('/home/ciaran/PACF_DAM.jpg', format='jpg', dpi=300)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d813e3",
   "metadata": {},
   "source": [
    "Partial Autocorrelation Function for the BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e5f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import datetime as dt\n",
    "\n",
    "# Define date format for parsing\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "\n",
    "# Function to parse dates\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "\n",
    "# Read CSV without frequency information and parse dates\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "# Drop non-numeric columns and handle missing data\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = dat.bfill(axis='rows').ffill(axis='rows')._get_numeric_data()\n",
    "\n",
    "# Select the relevant time series for analysis\n",
    "Y = dat.iloc[:, 0:1]  # EURPrices column\n",
    "X = dat.iloc[:, 16:]  # Other variables\n",
    "X = pd.concat([\n",
    "    X.iloc[:, :1],     # lag_-3x1\n",
    "    X.iloc[:, 49:50],  # lag_-3x2\n",
    "    X.iloc[:, 98:99],  # lag_-2x3\n",
    "    X.iloc[:, 147:148],# lag_0x4\n",
    "    X.iloc[:, 148:149],# lag_0x5\n",
    "    X.iloc[:, 149:150],# lag_0x6\n",
    "    X.iloc[:, 197:198],# lag_2x7\n",
    "    X.iloc[:, 213:214],# lag_2x8\n",
    "    X.iloc[:, 229:230],# lag_2x9\n",
    "    X.iloc[:, 245:246],# lag_2x10\n",
    "    X.iloc[:, 261:262],# lag_2x11\n",
    "    X.iloc[:, 277:278] # lag_-2x12\n",
    "], axis=1)\n",
    "\n",
    "# Extract specific series for historical and future-looking data\n",
    "historical_data = {\n",
    "    'BMP': X['lag_-3x1'],\n",
    "    'BMV': X['lag_-3x2'],\n",
    "    'WDiff': X['lag_-2x3'],\n",
    "    'I': X['lag_-2x12'],\n",
    "    'DAM': X['lag_0x6']\n",
    "}\n",
    "\n",
    "future_data = {\n",
    "    'PHPN': X['lag_2x7'],\n",
    "    'PHI': X['lag_2x8'],\n",
    "    'PHFW': X['lag_2x9'],\n",
    "    'PHFD': X['lag_2x10'],\n",
    "    'DAM': X['lag_2x11']\n",
    "}\n",
    "\n",
    "# Function to plot PACF\n",
    "def plot_pacf(series, lags=48, ax=None):\n",
    "    \"\"\"\n",
    "    Plot the Partial Autocorrelation Function (PACF) for a given time series.\n",
    "\n",
    "    Parameters:\n",
    "    - series: The time series data to plot.\n",
    "    - lags: Number of lags to include in the plot.\n",
    "    - ax: Matplotlib axis object for plotting.\n",
    "    \"\"\"\n",
    "    sm.graphics.tsa.plot_pacf(series, lags=lags, ax=ax, method='ywm', color='blue')\n",
    "    ax.set_xlabel('Lag', fontsize=12)\n",
    "    ax.set_ylabel('PACF', fontsize=12)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax.set_title(f'PACF - {series.name}', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Create subplots for historical and future-looking data\n",
    "fig, axes = plt.subplots(2, 4, figsize=(36, 12), sharey=True)\n",
    "fig.suptitle('Partial Autocorrelation Function Analysis for Balancing Market Dataset', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot PACF for historical data (first row)\n",
    "plot_pacf(historical_data['BMP'], lags=48, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('BM Prices', fontsize=14, fontweight='bold')\n",
    "\n",
    "plot_pacf(historical_data['BMV'], lags=48, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('BM Volume', fontsize=14, fontweight='bold')\n",
    "\n",
    "plot_pacf(historical_data['WDiff'], lags=48, ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Forecast Wind - Actual Wind', fontsize=14, fontweight='bold')\n",
    "\n",
    "plot_pacf(historical_data['I'], lags=48, ax=axes[0, 3])\n",
    "axes[0, 3].set_title('Interconnector Values', fontsize=14, fontweight='bold')\n",
    "\n",
    "# plot_pacf(historical_data['DAM'], lags=48, ax=axes[0, 4])\n",
    "# axes[0, 4].set_title('DAM', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot PACF for future-looking data (second row)\n",
    "plot_pacf(future_data['PHPN'], lags=48, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Physical Notifications Volume', fontsize=14, fontweight='bold')\n",
    "\n",
    "plot_pacf(future_data['PHI'], lags=48, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Net Interconnector Schedule', fontsize=14, fontweight='bold')\n",
    "\n",
    "plot_pacf(future_data['PHFW'], lags=48, ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Renewable Forecast', fontsize=14, fontweight='bold')\n",
    "\n",
    "plot_pacf(future_data['PHFD'], lags=48, ax=axes[1, 3])\n",
    "axes[1, 3].set_title('Demand Forecast', fontsize=14, fontweight='bold')\n",
    "\n",
    "# DAM is included twice in both historical and future data; use a distinct title for clarity\n",
    "# plot_pacf(future_data['DAM'], lags=48, ax=axes[1, 4])\n",
    "# axes[1, 4].set_title('DAM (Future)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Adjust layout and save the figure\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "# Save the figure as a JPG file\n",
    "plt.savefig('/home/ciaran/BM_PACF.jpg', format='jpg', dpi=500)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4faec03",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0a009b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb47ed",
   "metadata": {},
   "source": [
    "Serial Correlation Tests:\n",
    "\n",
    "1.) Ljung-Box Test\n",
    "\n",
    "2.) Breusch-Godfrey Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760b011b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96473055",
   "metadata": {},
   "source": [
    "Ljung-Box Test for the Day-Ahead Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ff07fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import acorr_breusch_godfrey\n",
    "\n",
    "# Define the date format for parsing\n",
    "date_format = \"%d/%m/%Y %H:%M\"\n",
    "date_parse = lambda date: pd.to_datetime(date, format=date_format)\n",
    "\n",
    "# Load the dataset\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/DAM_VAR_1-3.csv\", index_col=\"DeliveryPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "Y = dat.iloc[:, 0:1]  # The first column as the dependent variable\n",
    "X = dat.iloc[:, 24:]  # Select all independent variables from the 24th column onward\n",
    "X = pd.concat([X.iloc[:, :1], X.iloc[:, 144:145], X.iloc[:, 168:169], X.iloc[:, 312:313], X.iloc[:, 336:337]], axis=1)  # Select specific columns\n",
    "\n",
    "# Handling missing values by filling with the mean\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit Lasso model\n",
    "lasso = Lasso(alpha=0.01)  # Regularization parameter\n",
    "lasso.fit(X_train, Y_train)\n",
    "\n",
    "# Predict and calculate residuals for Lasso\n",
    "Y_pred_lasso = lasso.predict(X_test)\n",
    "Y_pred_lasso = pd.Series(Y_pred_lasso, index=Y_test.index)  # Convert predictions to a Series with the same index as Y_test\n",
    "residuals_lasso = Y_test.iloc[:, 0] - Y_pred_lasso  # Ensure both are Series with the same length\n",
    "\n",
    "# Perform Ljung-Box test on Lasso residuals\n",
    "lags = 24  # Number of lags for the test\n",
    "ljung_box_result_lasso = sm.stats.acorr_ljungbox(residuals_lasso, lags=lags, return_df=True)\n",
    "\n",
    "# Select the maximum lag's statistic and p-value as a single summary value\n",
    "max_lag_stat = ljung_box_result_lasso.iloc[-1]  # Taking the last row for lag 24\n",
    "\n",
    "print(\"Ljung-Box Test Summary for Lasso Model at Maximum:\")\n",
    "print(f\"Ljung-Box Statistic Lag (24): {max_lag_stat['lb_stat']:.4f}, p-value: {max_lag_stat['lb_pvalue']:.4f}\")\n",
    "\n",
    "# Perform Ljung-Box test on Lasso residuals\n",
    "lags = 48  # Number of lags for the test\n",
    "ljung_box_result_lasso = sm.stats.acorr_ljungbox(residuals_lasso, lags=lags, return_df=True)\n",
    "\n",
    "# Select the maximum lag's statistic and p-value as a single summary value\n",
    "max_lag_stat = ljung_box_result_lasso.iloc[-1]  # Taking the last row for lag 24\n",
    "\n",
    "print(\"Ljung-Box Test Summary for Lasso Model at Maximum:\")\n",
    "print(f\"Ljung-Box Statistic Lag (48): {max_lag_stat['lb_stat']:.4f}, p-value: {max_lag_stat['lb_pvalue']:.4f}\")\n",
    "\n",
    "# Perform Ljung-Box test on Lasso residuals\n",
    "lags = 96  # Number of lags for the test\n",
    "ljung_box_result_lasso = sm.stats.acorr_ljungbox(residuals_lasso, lags=lags, return_df=True)\n",
    "\n",
    "# Select the maximum lag's statistic and p-value as a single summary value\n",
    "max_lag_stat = ljung_box_result_lasso.iloc[-1]  # Taking the last row for lag 24\n",
    "\n",
    "print(\"Ljung-Box Test Summary for Lasso Model at Maximum:\")\n",
    "print(f\"Ljung-Box Statistic Lag (96): {max_lag_stat['lb_stat']:.4f}, p-value: {max_lag_stat['lb_pvalue']:.4f}\")\n",
    "\n",
    "\n",
    "# Perform Ljung-Box test on Lasso residuals\n",
    "lags = 167  # Number of lags for the test\n",
    "ljung_box_result_lasso = sm.stats.acorr_ljungbox(residuals_lasso, lags=lags, return_df=True)\n",
    "\n",
    "# Select the maximum lag's statistic and p-value as a single summary value\n",
    "max_lag_stat = ljung_box_result_lasso.iloc[-1]  # Taking the last row for lag 24\n",
    "\n",
    "print(\"Ljung-Box Test Summary for Lasso Model at Maximum:\")\n",
    "print(f\"Ljung-Box Statistic Lag (167): {max_lag_stat['lb_stat']:.4f}, p-value: {max_lag_stat['lb_pvalue']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551d7a0e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd60ff10",
   "metadata": {},
   "source": [
    "Breusch-Godfrey test Day-Ahead Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b0fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags=24\n",
    "ols_model = sm.OLS(Y_train, sm.add_constant(X_train)).fit()\n",
    "bg_test = acorr_breusch_godfrey(ols_model, nlags=lags)\n",
    "bg_stat, bg_pvalue, _, _ = bg_test\n",
    "print(\"\\nBreusch-Godfrey Test Results for Lasso Model Residuals:\")\n",
    "print(f\"Breusch-Godfrey Statistic lags 24: {bg_stat:.4f}, p-value: {bg_pvalue:.4f}\")\n",
    "\n",
    "lags=48\n",
    "ols_model = sm.OLS(Y_train, sm.add_constant(X_train)).fit()\n",
    "bg_test = acorr_breusch_godfrey(ols_model, nlags=lags)\n",
    "bg_stat, bg_pvalue, _, _ = bg_test\n",
    "print(\"\\nBreusch-Godfrey Test Results for Lasso Model Residuals:\")\n",
    "print(f\"Breusch-Godfrey Statistic lags 48: {bg_stat:.4f}, p-value: {bg_pvalue:.4f}\")\n",
    "\n",
    "lags=96\n",
    "ols_model = sm.OLS(Y_train, sm.add_constant(X_train)).fit()\n",
    "bg_test = acorr_breusch_godfrey(ols_model, nlags=lags)\n",
    "bg_stat, bg_pvalue, _, _ = bg_test\n",
    "print(\"\\nBreusch-Godfrey Test Results for Lasso Model Residuals:\")\n",
    "print(f\"Breusch-Godfrey Statistic lags 96: {bg_stat:.4f}, p-value: {bg_pvalue:.4f}\")\n",
    "\n",
    "lags=167\n",
    "ols_model = sm.OLS(Y_train, sm.add_constant(X_train)).fit()\n",
    "bg_test = acorr_breusch_godfrey(ols_model, nlags=lags)\n",
    "bg_stat, bg_pvalue, _, _ = bg_test\n",
    "print(\"\\nBreusch-Godfrey Test Results for Lasso Model Residuals :\")\n",
    "print(f\"Breusch-Godfrey Statistic lags 167: {bg_stat:.4f}, p-value: {bg_pvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4276e6",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec42bb9",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97279956",
   "metadata": {},
   "source": [
    "Ljung-Box Test for the Balancing Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8cb44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import acorr_breusch_godfrey\n",
    "\n",
    "# Define date format for parsing\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "\n",
    "# Function to parse dates\n",
    "date_format = lambda date: dt.datetime.strptime(date, date_format)\n",
    "\n",
    "# Read CSV without frequency information and parse dates\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_format=date_format)\n",
    "\n",
    "# Specify targets for forecasting\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "# Select the relevant time series for analysis\n",
    "Y = dat.iloc[:, 0:1]  # EURPrices column\n",
    "X = dat.iloc[:, 16:]  # Other variables\n",
    "X = pd.concat([X.iloc[:, :1], X.iloc[:, 49:50], X.iloc[:, 98:99], X.iloc[:, 147:148], X.iloc[:, 148:149],\n",
    "X.iloc[:, 149:150], X.iloc[:, 197:198], X.iloc[:, 213:214], X.iloc[:, 229:230], \n",
    "X.iloc[:, 245:246], X.iloc[:, 261:262], X.iloc[:, 277:278]], axis=1)\n",
    "\n",
    "# Handling missing values by filling with the mean\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit Lasso model\n",
    "lasso = Lasso(alpha=0.01)  # Regularization parameter\n",
    "lasso.fit(X_train, Y_train)\n",
    "\n",
    "# Predict and calculate residuals for Lasso\n",
    "Y_pred_lasso = lasso.predict(X_test)\n",
    "Y_pred_lasso = pd.Series(Y_pred_lasso, index=Y_test.index)  # Convert predictions to a Series with the same index as Y_test\n",
    "residuals_lasso = Y_test.iloc[:, 0] - Y_pred_lasso  # Ensure both are Series with the same length\n",
    "\n",
    "lags = 16  # Number of lags for the test\n",
    "ljung_box_result_lasso = sm.stats.acorr_ljungbox(residuals_lasso, lags=lags, return_df=True)\n",
    "max_lag_stat = ljung_box_result_lasso.iloc[-1]  # Taking the last row for lag 24\n",
    "print(\"Ljung-Box Test Summary for Lasso Model at Maximum:\")\n",
    "print(f\"Ljung-Box Statistic Lag (16): {max_lag_stat['lb_stat']:.4f}, p-value: {max_lag_stat['lb_pvalue']:.4f}\")\n",
    "\n",
    "lags = 32  # Number of lags for the test\n",
    "ljung_box_result_lasso = sm.stats.acorr_ljungbox(residuals_lasso, lags=lags, return_df=True)\n",
    "max_lag_stat = ljung_box_result_lasso.iloc[-1]  # Taking the last row for lag 24\n",
    "print(\"Ljung-Box Test Summary for Lasso Model at Maximum:\")\n",
    "print(f\"Ljung-Box Statistic Lag (32): {max_lag_stat['lb_stat']:.4f}, p-value: {max_lag_stat['lb_pvalue']:.4f}\")\n",
    "\n",
    "lags = 48  # Number of lags for the test\n",
    "ljung_box_result_lasso = sm.stats.acorr_ljungbox(residuals_lasso, lags=lags, return_df=True)\n",
    "max_lag_stat = ljung_box_result_lasso.iloc[-1]  # Taking the last row for lag 24\n",
    "print(\"Ljung-Box Test Summary for Lasso Model at Maximum:\")\n",
    "print(f\"Ljung-Box Statistic Lag (48): {max_lag_stat['lb_stat']:.4f}, p-value: {max_lag_stat['lb_pvalue']:.4f}\")\n",
    "\n",
    "lags = 96  # Number of lags for the test\n",
    "ljung_box_result_lasso = sm.stats.acorr_ljungbox(residuals_lasso, lags=lags, return_df=True)\n",
    "max_lag_stat = ljung_box_result_lasso.iloc[-1]  # Taking the last row for lag 24\n",
    "print(\"Ljung-Box Test Summary for Lasso Model at Maximum:\")\n",
    "print(f\"Ljung-Box Statistic Lag (96): {max_lag_stat['lb_stat']:.4f}, p-value: {max_lag_stat['lb_pvalue']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb250f02",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514f27e5",
   "metadata": {},
   "source": [
    "Breusch-Godfrey test Balancing Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bddf7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags=16\n",
    "ols_model = sm.OLS(Y_train, sm.add_constant(X_train)).fit()\n",
    "bg_test = acorr_breusch_godfrey(ols_model, nlags=lags)\n",
    "bg_stat, bg_pvalue, _, _ = bg_test\n",
    "print(\"\\nBreusch-Godfrey Test Results for Lasso Model Residuals:\")\n",
    "print(f\"Breusch-Godfrey Statistic Lag (16): {bg_stat:.4f}, p-value: {bg_pvalue:.4f}\")\n",
    "\n",
    "lags=32\n",
    "ols_model = sm.OLS(Y_train, sm.add_constant(X_train)).fit()\n",
    "bg_test = acorr_breusch_godfrey(ols_model, nlags=lags)\n",
    "bg_stat, bg_pvalue, _, _ = bg_test\n",
    "print(\"\\nBreusch-Godfrey Test Results for Lasso Model Residuals:\")\n",
    "print(f\"Breusch-Godfrey Statistic Lag (32): {bg_stat:.4f}, p-value: {bg_pvalue:.4f}\")\n",
    "\n",
    "lags=48\n",
    "ols_model = sm.OLS(Y_train, sm.add_constant(X_train)).fit()\n",
    "bg_test = acorr_breusch_godfrey(ols_model, nlags=lags)\n",
    "bg_stat, bg_pvalue, _, _ = bg_test\n",
    "print(\"\\nBreusch-Godfrey Test Results for Lasso Model Residuals:\")\n",
    "print(f\"Breusch-Godfrey Statistic Lag (48): {bg_stat:.4f}, p-value: {bg_pvalue:.4f}\")\n",
    "\n",
    "lags=96\n",
    "ols_model = sm.OLS(Y_train, sm.add_constant(X_train)).fit()\n",
    "bg_test = acorr_breusch_godfrey(ols_model, nlags=lags)\n",
    "bg_stat, bg_pvalue, _, _ = bg_test\n",
    "print(\"\\nBreusch-Godfrey Test Results for Lasso Model Residuals:\")\n",
    "print(f\"Breusch-Godfrey Statistic Lag (96): {bg_stat:.4f}, p-value: {bg_pvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83e9ce",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e270c7",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d9837c",
   "metadata": {},
   "source": [
    "Heteroscedasticity Tests-\n",
    "\n",
    "Breusch-Pagan Test\n",
    "\n",
    "White Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91169ad3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438735a7",
   "metadata": {},
   "source": [
    "Breusch-Pagan Test for the Day-Ahead Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94882ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "# Load the data\n",
    "date_format = \"%d/%m/%Y %H:%M\"\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/DAM_VAR_1-3.csv\", index_col=\"DeliveryPeriod\", parse_dates=True, date_format=date_format)\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "# Define the dependent (Y) and independent (X) variables\n",
    "Y = dat.iloc[:, 0]  # Dependent variable (first column)\n",
    "X = dat.iloc[:, 24:]  # Independent variables starting from the 24th column\n",
    "\n",
    "# Selecting specific columns from the independent variables\n",
    "X = pd.concat([X.iloc[:, :1], X.iloc[:, 144:145], X.iloc[:, 168:169], X.iloc[:, 312:313], X.iloc[:, 336:337]], axis=1)\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Standardize the features (Lasso is sensitive to scale)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit Lasso regression with a specific alpha value (regularization strength)\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "lasso_model.fit(X_train_scaled, Y_train)  # No need to scale Y_train, model will predict on the same scale\n",
    "\n",
    "# Calculate residuals on the TRAINING set\n",
    "Y_train_pred = lasso_model.predict(X_train_scaled)\n",
    "residuals_train = Y_train - Y_train_pred  # Correct because Y_train was not scaled\n",
    "\n",
    "# Perform the Breusch-Pagan Test\n",
    "# Add a constant to the independent variables (required for the test)\n",
    "X_train_with_const = sm.add_constant(X_train_scaled)\n",
    "\n",
    "# Perform the Breusch-Pagan test on the training set residuals\n",
    "bp_test = het_breuschpagan(residuals_train, X_train_with_const)\n",
    "\n",
    "# Extract the test results\n",
    "bp_stat, bp_pvalue, fvalue, f_pvalue = bp_test\n",
    "\n",
    "# Print the Breusch-Pagan Test results\n",
    "print(f\"Breusch-Pagan Test Statistic (LM): {bp_stat}\")\n",
    "print(f\"p-value (LM test): {bp_pvalue}\")\n",
    "print(f\"F-statistic: {fvalue}\")\n",
    "print(f\"p-value (F-test): {f_pvalue}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca9bb1a",
   "metadata": {},
   "source": [
    "Breusch-Pagan Test for the Balancing Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c87d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "# Load the data\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_format = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_format=date_format)\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "# Define dependent (Y) and independent (X) variables\n",
    "Y = dat.iloc[:, 0:1]  # EURPrices column\n",
    "X = dat.iloc[:, 16:]  # Other variables\n",
    "X = pd.concat([X.iloc[:, :1], X.iloc[:, 49:50], X.iloc[:, 98:99], X.iloc[:, 147:148], X.iloc[:, 148:149],\n",
    "               X.iloc[:, 149:150], X.iloc[:, 197:198], X.iloc[:, 213:214], X.iloc[:, 229:230], \n",
    "               X.iloc[:, 245:246], X.iloc[:, 261:262], X.iloc[:, 277:278]], axis=1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Standardize the features (Lasso is sensitive to scale)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit Lasso regression\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "lasso_model.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# Calculate residuals on the training set\n",
    "Y_train_pred = lasso_model.predict(X_train_scaled)\n",
    "\n",
    "# Convert Y_train to a 1D array for compatibility with Y_train_pred\n",
    "Y_train_flat = Y_train.values.ravel()  # Flatten Y_train\n",
    "\n",
    "# Calculate residuals\n",
    "residuals_train = Y_train_flat - Y_train_pred\n",
    "\n",
    "# Perform the Breusch-Pagan Test\n",
    "# Add a constant to the independent variables (required for the test)\n",
    "X_train_with_const = sm.add_constant(X_train_scaled)\n",
    "\n",
    "# Perform the Breusch-Pagan test on the training set residuals\n",
    "bp_test = het_breuschpagan(residuals_train, X_train_with_const)\n",
    "\n",
    "# Extract the test results\n",
    "bp_stat, bp_pvalue, fvalue, f_pvalue = bp_test\n",
    "\n",
    "# Print the Breusch-Pagan Test results\n",
    "print(f\"Breusch-Pagan Test Statistic (LM): {bp_stat}\")\n",
    "print(f\"p-value (LM test): {bp_pvalue}\")\n",
    "print(f\"F-statistic: {fvalue}\")\n",
    "print(f\"p-value (F-test): {f_pvalue}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493af725",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1db2296",
   "metadata": {},
   "source": [
    "White Test for the Day-Ahead Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f79d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "\n",
    "# Step 1: Load the data\n",
    "date_format = \"%d/%m/%Y %H:%M\"\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/DAM_VAR_1-3.csv\", index_col=\"DeliveryPeriod\", parse_dates=True, date_format=date_format)\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "# Step 2: Define the dependent (Y) and independent (X) variables\n",
    "Y = dat.iloc[:, 0]  # Dependent variable (first column)\n",
    "X = dat.iloc[:, 24:]  # Independent variables starting from the 24th column\n",
    "\n",
    "# Step 3: Selecting specific columns from the independent variables\n",
    "X = pd.concat([X.iloc[:, :1], X.iloc[:, 144:145], X.iloc[:, 168:169], X.iloc[:, 312:313], X.iloc[:, 336:337]], axis=1)\n",
    "\n",
    "# Step 4: Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Standardize the features (Lasso is sensitive to scale)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert the scaled X_train back to a DataFrame to keep the index alignment with Y_train\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n",
    "\n",
    "# Step 6: Fit Lasso regression with a specific alpha value (regularization strength)\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "lasso_model.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# Step 7: Calculate residuals on the TRAINING set\n",
    "Y_train_pred = lasso_model.predict(X_train_scaled)\n",
    "residuals_train = Y_train - Y_train_pred\n",
    "\n",
    "# Step 8: Create auxiliary regression dataset with squared terms and interaction terms\n",
    "def create_white_test_data(X):\n",
    "    X_df = pd.DataFrame(X)  # Input data already has a constant added if necessary\n",
    "    \n",
    "    # Add squared terms\n",
    "    squared_terms = {f'{col}^2': X_df[col] ** 2 for col in X_df.columns}\n",
    "    \n",
    "    # Add interaction terms\n",
    "    interaction_terms = {f'{col1}*{col2}': X_df[col1] * X_df[col2] \n",
    "                         for col1, col2 in combinations(X_df.columns, 2)}\n",
    "    \n",
    "    # Combine original, squared, and interaction terms into a single DataFrame\n",
    "    X_expanded = pd.concat([X_df, pd.DataFrame(squared_terms), pd.DataFrame(interaction_terms)], axis=1)\n",
    "\n",
    "    return X_expanded\n",
    "\n",
    "# Step 9: Run the adapted White Test\n",
    "def run_adapted_white_test(X, residuals):\n",
    "    # Create the expanded auxiliary regression dataset\n",
    "    X_auxiliary = create_white_test_data(X)\n",
    "    \n",
    "    # Fit the auxiliary regression with squared residuals as the target\n",
    "    auxiliary_model = sm.OLS(residuals**2, sm.add_constant(X_auxiliary)).fit()\n",
    "    \n",
    "    # Calculate the White test statistic\n",
    "    n = X_auxiliary.shape[0]  # Number of observations\n",
    "    R_squared = auxiliary_model.rsquared\n",
    "    white_statistic = n * R_squared  # White test statistic (LM test)\n",
    "\n",
    "    # Degrees of freedom for chi-square test\n",
    "    df = X_auxiliary.shape[1]  # Degrees of freedom for the test statistic\n",
    "    \n",
    "    # p-value from chi-square distribution using scipy\n",
    "    p_value = stats.chi2.sf(white_statistic, df)\n",
    "    \n",
    "    return white_statistic, p_value, auxiliary_model\n",
    "\n",
    "# Step 10: Apply the adapted White Test on the training data\n",
    "white_stat, p_value, auxiliary_model = run_adapted_white_test(X_train_scaled, residuals_train)\n",
    "\n",
    "# Step 11: Print the results\n",
    "print(f\"White Test Statistic: {white_stat}\")\n",
    "print(f\"p-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e701fb",
   "metadata": {},
   "source": [
    "White Test for the Balancing Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ce64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "\n",
    "# Step 1: Load the data\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True)\n",
    "dat = dat.drop([\"index\"], axis=1, errors='ignore')  # Drop 'index' column if it exists\n",
    "dat = dat.bfill(axis='rows').ffill(axis='rows')  # Backfill and forward fill missing values\n",
    "dat = dat._get_numeric_data()  # Keep only numeric data\n",
    "\n",
    "# Step 2: Define dependent (Y) and independent (X) variables\n",
    "Y = dat.iloc[:, 0]  # Assuming the first column represents the dependent variable (e.g., EURPrices)\n",
    "X = dat.iloc[:, 16:]  # Starting from the 17th column as independent variables\n",
    "\n",
    "# Step 3: Selecting specific columns from the independent variables\n",
    "X = pd.concat([X.iloc[:, :1], X.iloc[:, 49:50], X.iloc[:, 98:99], X.iloc[:, 147:148], X.iloc[:, 148:149],\n",
    "               X.iloc[:, 149:150], X.iloc[:, 197:198], X.iloc[:, 213:214], X.iloc[:, 229:230], \n",
    "               X.iloc[:, 245:246], X.iloc[:, 261:262], X.iloc[:, 277:278]], axis=1)\n",
    "\n",
    "# Step 4: Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Standardize the features (Lasso is sensitive to scale)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert the scaled X_train back to a DataFrame to keep the index alignment with Y_train\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n",
    "\n",
    "# Step 6: Fit Lasso regression with a specific alpha value (regularization strength)\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "lasso_model.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# Step 7: Calculate residuals on the TRAINING set\n",
    "Y_train_pred = lasso_model.predict(X_train_scaled)\n",
    "residuals_train = Y_train - Y_train_pred\n",
    "\n",
    "# Step 8: Create auxiliary regression dataset with squared terms and interaction terms\n",
    "def create_white_test_data(X):\n",
    "    X_df = pd.DataFrame(X)  # Input data already has a constant added if necessary\n",
    "    \n",
    "    # Add squared terms\n",
    "    squared_terms = {f'{col}^2': X_df[col] ** 2 for col in X_df.columns}\n",
    "    \n",
    "    # Add interaction terms\n",
    "    interaction_terms = {f'{col1}*{col2}': X_df[col1] * X_df[col2] \n",
    "                         for col1, col2 in combinations(X_df.columns, 2)}\n",
    "    \n",
    "    # Combine original, squared, and interaction terms into a single DataFrame\n",
    "    X_expanded = pd.concat([X_df, pd.DataFrame(squared_terms), pd.DataFrame(interaction_terms)], axis=1)\n",
    "\n",
    "    return X_expanded\n",
    "\n",
    "# Step 9: Run the adapted White Test\n",
    "def run_adapted_white_test(X, residuals):\n",
    "    # Create the expanded auxiliary regression dataset\n",
    "    X_auxiliary = create_white_test_data(X)\n",
    "    \n",
    "    # Fit the auxiliary regression with squared residuals as the target\n",
    "    auxiliary_model = sm.OLS(residuals**2, sm.add_constant(X_auxiliary)).fit()\n",
    "    \n",
    "    # Calculate the White test statistic\n",
    "    n = X_auxiliary.shape[0]  # Number of observations\n",
    "    R_squared = auxiliary_model.rsquared\n",
    "    white_statistic = n * R_squared  # White test statistic (LM test)\n",
    "\n",
    "    # Degrees of freedom for chi-square test\n",
    "    df = X_auxiliary.shape[1]  # Degrees of freedom for the test statistic\n",
    "    \n",
    "    # p-value from chi-square distribution using scipy\n",
    "    p_value = stats.chi2.sf(white_statistic, df)\n",
    "    \n",
    "    return white_statistic, p_value, auxiliary_model\n",
    "\n",
    "# Step 10: Apply the adapted White Test on the training data\n",
    "white_stat, p_value, auxiliary_model = run_adapted_white_test(X_train_scaled, residuals_train)\n",
    "\n",
    "# Step 11: Print the results\n",
    "print(f\"White Test Statistic: {white_stat}\")\n",
    "print(f\"p-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30050a37",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47384bb7",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c43b72",
   "metadata": {},
   "source": [
    "Multicollinearity Tests-\n",
    "\n",
    "Variance Inflation Factor(VIF)\n",
    "\n",
    "Condition Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622b89fe",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb205a",
   "metadata": {},
   "source": [
    "Variance Inflation Factor and Condition Index for the Day-Ahead Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e8214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from numpy.linalg import eigvals\n",
    "\n",
    "# Load the data\n",
    "date_format = \"%d/%m/%Y %H:%M\"\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/DAM_VAR_1-3.csv\", index_col=\"DeliveryPeriod\", parse_dates=True, date_format=date_format)\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "# Handle missing or infinite values\n",
    "dat.replace([np.inf, -np.inf], np.nan, inplace=True)  # Replace infs with NaNs\n",
    "dat.dropna(inplace=True)  # Drop rows with NaNs\n",
    "\n",
    "# Separate the dependent and independent variables\n",
    "Y = dat.iloc[:, 0:1]  # The first column as the dependent variable\n",
    "X = dat.iloc[:, 24:]  # Select all independent variables from the 24th column onward\n",
    "X = pd.concat([X.iloc[:, :1], X.iloc[:, 144:145], X.iloc[:, 312:313]], axis=1)  # Select specific columns\n",
    "\n",
    "# Add a constant to the independent variables for the regression\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Calculate Variance Inflation Factor (VIF) for each selected independent variable\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Variable\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(\"Variance Inflation Factor (VIF) results:\")\n",
    "print(vif_data)\n",
    "\n",
    "# Scale the independent variables for Condition Index calculation (excluding the constant)\n",
    "X_scaled = X.copy()\n",
    "X_scaled.iloc[:, 1:] = (X.iloc[:, 1:] - X.iloc[:, 1:].mean()) / X.iloc[:, 1:].std(ddof=0)\n",
    "\n",
    "# Check for NaN or inf values in the scaled data\n",
    "if np.any(np.isnan(X_scaled)) or np.any(np.isinf(X_scaled)):\n",
    "    print(\"Warning: Scaled data contains NaN or inf values.\")\n",
    "    print(\"\\nRows with NaN or inf values in X_scaled:\")\n",
    "    print(X_scaled[np.isnan(X_scaled).any(axis=1) | np.isinf(X_scaled).any(axis=1)])\n",
    "else:\n",
    "    # Compute the eigenvalues of the scaled independent variable matrix\n",
    "    eigenvalues = eigvals(np.dot(X_scaled.iloc[:, 1:].T, X_scaled.iloc[:, 1:]))\n",
    "\n",
    "    # Calculate the Condition Index for each eigenvalue\n",
    "    condition_index = np.sqrt(eigenvalues.max() / eigenvalues)\n",
    "\n",
    "    print(\"\\nCondition Index results:\")\n",
    "    for i, ci in enumerate(condition_index):\n",
    "        print(f\"Condition Index {i+1}: {ci:.2f}\")\n",
    "\n",
    "# Interpretation:\n",
    "# - High VIF (> 10) indicates significant multicollinearity.\n",
    "# - Condition Index above 30 suggests severe multicollinearity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96505b77",
   "metadata": {},
   "source": [
    "Variance Inflation Factor and Condition Index for the Balancing Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1200955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from numpy.linalg import eigvals\n",
    "\n",
    "# Define date format for parsing\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "\n",
    "# Function to parse dates\n",
    "date_format = lambda date: dt.datetime.strptime(date, date_format)\n",
    "\n",
    "# Read CSV without frequency information and parse dates\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_format=date_format)\n",
    "\n",
    "# Specify targets for forecasting\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "# Select the relevant time series for analysis\n",
    "Y = dat.iloc[:, 0:1]  # EURPrices column\n",
    "X = dat.iloc[:, 16:]  # Other variables\n",
    "X = pd.concat([X.iloc[:, :1], X.iloc[:, 49:50], X.iloc[:, 98:99], X.iloc[:, 147:148], X.iloc[:, 148:149],\n",
    "X.iloc[:, 149:150], X.iloc[:, 197:198], X.iloc[:, 213:214], X.iloc[:, 229:230], \n",
    "X.iloc[:, 245:246], X.iloc[:, 261:262], X.iloc[:, 277:278]], axis=1)\n",
    "\n",
    "# Add a constant to the independent variables for the regression\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Calculate Variance Inflation Factor (VIF) for each selected independent variable\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Variable\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(\"Variance Inflation Factor (VIF) results:\")\n",
    "print(vif_data)\n",
    "\n",
    "# Scale the independent variables for Condition Index calculation (excluding the constant)\n",
    "X_scaled = X.copy()\n",
    "X_scaled.iloc[:, 1:] = (X.iloc[:, 1:] - X.iloc[:, 1:].mean()) / X.iloc[:, 1:].std(ddof=0)\n",
    "\n",
    "# Check for NaN or inf values in the scaled data\n",
    "if np.any(np.isnan(X_scaled)) or np.any(np.isinf(X_scaled)):\n",
    "    print(\"Warning: Scaled data contains NaN or inf values.\")\n",
    "    print(\"\\nRows with NaN or inf values in X_scaled:\")\n",
    "    print(X_scaled[np.isnan(X_scaled).any(axis=1) | np.isinf(X_scaled).any(axis=1)])\n",
    "else:\n",
    "    # Compute the eigenvalues of the scaled independent variable matrix\n",
    "    eigenvalues = eigvals(np.dot(X_scaled.iloc[:, 1:].T, X_scaled.iloc[:, 1:]))\n",
    "\n",
    "    # Calculate the Condition Index for each eigenvalue\n",
    "    condition_index = np.sqrt(eigenvalues.max() / eigenvalues)\n",
    "\n",
    "    print(\"\\nCondition Index results:\")\n",
    "    for i, ci in enumerate(condition_index):\n",
    "        print(f\"Condition Index {i+1}: {ci:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa0feb3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c64d1",
   "metadata": {},
   "source": [
    "Correlation Analysis-\n",
    "\n",
    "Pearson Correlation Coefficient\n",
    "\n",
    "Spearman Rank Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e75c6c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5b704d",
   "metadata": {},
   "source": [
    "Pearson Correlation Coefficient for the Day-Ahead Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9011376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "date_format = \"%d/%m/%Y %H:%M\"\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/DAM_VAR_1-3.csv\", index_col=\"DeliveryPeriod\", parse_dates=True, date_format=date_format)\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "# Handle missing or infinite values\n",
    "dat.replace([np.inf, -np.inf], np.nan, inplace=True)  # Replace infs with NaNs\n",
    "dat.dropna(inplace=True)  # Drop rows with NaNs\n",
    "\n",
    "# Select dependent and independent variables\n",
    "Y = dat.iloc[:, 0:1]  # The first column as the dependent variable\n",
    "X = dat.iloc[:, 24:]  # Select all independent variables from the 24th column onward\n",
    "X = pd.concat([X.iloc[:, 144:145], X.iloc[:, 312:313]], axis=1)  # Select specific columns\n",
    "\n",
    "# Rename columns for better readability\n",
    "Y.columns = ['DAM Prices']  # Rename dependent variable\n",
    "X.columns = ['Wind Forecast', 'Demand Forecast']  # Rename independent variables\n",
    "\n",
    "# Combine Y and X for correlation analysis\n",
    "combined_data = pd.concat([Y, X], axis=1)\n",
    "\n",
    "# Calculate the Pearson correlation matrix\n",
    "correlation_matrix = combined_data.corr(method='pearson')\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Pearson Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Create the heatmap plot\n",
    "plt.figure(figsize=(12, 10))  # Increase figure size for clarity\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1,\n",
    "            xticklabels=combined_data.columns, yticklabels=combined_data.columns, cbar_kws={\"shrink\": .75})  # Add color bar\n",
    "plt.title(\"Pearson Correlation Heatmap: Day-Ahead Market vs Forecast Variables\", fontsize=16, fontweight='bold', pad=20)  # More informative title\n",
    "plt.xticks(fontsize=12, rotation=45, ha='right')  # Rotate and adjust x-axis labels for better readability\n",
    "plt.yticks(fontsize=12)  # Adjust y-axis labels font size\n",
    "plt.xlabel('Variables', fontsize=14, labelpad=15)  # Custom x-axis label with padding\n",
    "plt.ylabel('Variables', fontsize=14, labelpad=15)  # Custom y-axis label with padding\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Ensures that labels and titles don't overlap with the figure\n",
    "plt.savefig('/home/ciaran/Pearson_Correlation_DAM.jpg', format='jpg', dpi=500)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c166e31",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2ec9f2",
   "metadata": {},
   "source": [
    "Pearson Correlation Coefficient for the Balancing Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd51e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "\n",
    "# Define date format for parsing\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "\n",
    "# Function to parse dates\n",
    "date_format = lambda date: dt.datetime.strptime(date, date_format)\n",
    "\n",
    "# Read CSV without frequency information and parse dates\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_format=date_format)\n",
    "\n",
    "# Handle missing or infinite values\n",
    "dat = dat.drop([\"index\"], axis=1)  # Remove unnecessary 'index' column\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')  # Backward fill to handle missing values\n",
    "dat = dat.ffill(axis='rows')  # Forward fill to handle missing values\n",
    "dat = dat._get_numeric_data()  # Only keep numeric data\n",
    "\n",
    "# Select relevant time series for analysis\n",
    "Y = dat.iloc[:, 0:1]  # BM Prices as the dependent variable (first column)\n",
    "X = dat.iloc[:, 16:]  # Other independent variables (from 17th column onward)\n",
    "\n",
    "# Select specific columns for X and rename them for clarity\n",
    "X = pd.concat([X.iloc[:, :1], X.iloc[:, 49:50], X.iloc[:, 98:99], X.iloc[:, 147:148], X.iloc[:, 148:149],\n",
    "               X.iloc[:, 149:150], X.iloc[:, 197:198], X.iloc[:, 213:214], X.iloc[:, 229:230], \n",
    "               X.iloc[:, 245:246], X.iloc[:, 261:262], X.iloc[:, 277:278]], axis=1)\n",
    "\n",
    "# Rename columns for better understanding\n",
    "Y.columns = ['BM Prices']  # Dependent variable label\n",
    "X.columns = ['Past BM Prices', 'BM Volume', 'Wind Difference', 'Carbon Price', 'Gas Price',\n",
    "             'Past DAM Prices', 'Physical Notifications Volume', 'Net Interconnector Schedule', \n",
    "             'Renewable Forecast', 'Indep_Var_10', 'Demand Forecast', 'Future DAM Prices']  # Independent variable labels\n",
    "\n",
    "# Combine Y and X for correlation analysis\n",
    "combined_data = pd.concat([Y, X], axis=1)\n",
    "\n",
    "# Calculate the Pearson correlation matrix\n",
    "correlation_matrix = combined_data.corr(method='pearson')\n",
    "\n",
    "# Display the correlation matrix in the console\n",
    "print(\"Pearson Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Create the heatmap plot\n",
    "plt.figure(figsize=(14, 12))  # Increase figure size for clarity\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1,\n",
    "            xticklabels=combined_data.columns, yticklabels=combined_data.columns, cbar_kws={\"shrink\": .75})  # Add color bar\n",
    "plt.title(\"Pearson Correlation Heatmap: Balancing Market vs Forecast Variables\", fontsize=16, fontweight='bold', pad=20)  # Improved title\n",
    "plt.xticks(fontsize=12, rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.yticks(fontsize=12)  # Adjust font size of y-axis labels\n",
    "plt.xlabel('Variables', fontsize=14, labelpad=15)  # Custom x-axis label with padding\n",
    "plt.ylabel('Variables', fontsize=14, labelpad=15)  # Custom y-axis label with padding\n",
    "\n",
    "# Show the plot with tight layout\n",
    "plt.tight_layout()  # Ensure no overlap of labels or titles\n",
    "plt.savefig('/home/ciaran/Pearson_Correlation_BM.jpg', format='jpg', dpi=500)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa499b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fc31ac",
   "metadata": {},
   "source": [
    "Spearman Rank Correlation for the Day-Ahead Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54160a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Load the data\n",
    "date_format = \"%d/%m/%Y %H:%M\"\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/DAM_VAR_1-3.csv\", index_col=\"DeliveryPeriod\", parse_dates=True, date_format=date_format)\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "# Handle missing or infinite values\n",
    "dat.replace([np.inf, -np.inf], np.nan, inplace=True)  # Replace infs with NaNs\n",
    "dat.dropna(inplace=True)  # Drop rows with NaNs\n",
    "\n",
    "Y = dat.iloc[:, 0:1]  \n",
    "X = dat.iloc[:, 24:]  \n",
    "X = pd.concat([X.iloc[:, 144:145], X.iloc[:, 312:313]], axis=1)  \n",
    "\n",
    "# Rename columns for better readability\n",
    "Y.columns = ['DAM Prices']  # Rename dependent variable\n",
    "X.columns = ['Wind Forecast', 'Demand Forecast']  # Rename independent variables\n",
    "\n",
    "# Combine Y and X for correlation analysis\n",
    "combined_data = pd.concat([Y, X], axis=1)\n",
    "\n",
    "# Initialize an empty DataFrame to store Spearman correlation coefficients\n",
    "spearman_corr_matrix = pd.DataFrame(index=combined_data.columns, columns=combined_data.columns)\n",
    "\n",
    "# Calculate Spearman correlation for each pair of variables\n",
    "for col1 in combined_data.columns:\n",
    "    for col2 in combined_data.columns:\n",
    "        corr, _ = spearmanr(combined_data[col1], combined_data[col2])\n",
    "        spearman_corr_matrix.loc[col1, col2] = corr\n",
    "\n",
    "# Convert correlation values to numeric type\n",
    "spearman_corr_matrix = spearman_corr_matrix.astype(float)\n",
    "\n",
    "# Display the Spearman correlation matrix\n",
    "print(\"Spearman Correlation Matrix:\")\n",
    "print(spearman_corr_matrix)\n",
    "\n",
    "# Create the heatmap plot for Spearman Rank Correlation\n",
    "plt.figure(figsize=(12, 10))  # Increase figure size for clarity\n",
    "sns.heatmap(spearman_corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1,\n",
    "            xticklabels=combined_data.columns, yticklabels=combined_data.columns, cbar_kws={\"shrink\": .75})  # Add color bar\n",
    "plt.title(\"Spearman Rank Correlation Heatmap: Day-Ahead Market vs Forecast Variables\", fontsize=16, fontweight='bold', pad=20)  # More informative title\n",
    "plt.xticks(fontsize=12, rotation=45, ha='right')  # Rotate and adjust x-axis labels for better readability\n",
    "plt.yticks(fontsize=12)  # Adjust y-axis labels font size\n",
    "plt.xlabel('Variables', fontsize=14, labelpad=15)  # Custom x-axis label with padding\n",
    "plt.ylabel('Variables', fontsize=14, labelpad=15)  # Custom y-axis label with padding\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Ensures that labels and titles don't overlap with the figure\n",
    "plt.savefig('/home/ciaran/Spearman_Correlation_DAM.jpg', format='jpg', dpi=500)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4f77cc",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75f9e33",
   "metadata": {},
   "source": [
    "Spearman Rank Correlation for the Balancing Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f07cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "\n",
    "# Define date format for parsing\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "\n",
    "# Function to parse dates\n",
    "date_format = lambda date: dt.datetime.strptime(date, date_format)\n",
    "\n",
    "# Read CSV without frequency information and parse dates\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_format=date_format)\n",
    "\n",
    "# Handle missing or infinite values\n",
    "dat = dat.drop([\"index\"], axis=1)  # Remove unnecessary 'index' column\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')  # Backward fill to handle missing values\n",
    "dat = dat.ffill(axis='rows')  # Forward fill to handle missing values\n",
    "dat = dat._get_numeric_data()  # Only keep numeric data\n",
    "\n",
    "# Select relevant time series for analysis\n",
    "Y = dat.iloc[:, 0:1]  # BM Prices as the dependent variable (first column)\n",
    "X = dat.iloc[:, 16:]  # Other independent variables (from 17th column onward)\n",
    "\n",
    "# Select specific columns for X and rename them for clarity\n",
    "X = pd.concat([X.iloc[:, :1], X.iloc[:, 49:50], X.iloc[:, 98:99], X.iloc[:, 147:148], X.iloc[:, 148:149],\n",
    "               X.iloc[:, 149:150], X.iloc[:, 197:198], X.iloc[:, 213:214], X.iloc[:, 229:230], \n",
    "               X.iloc[:, 245:246], X.iloc[:, 261:262], X.iloc[:, 277:278]], axis=1)\n",
    "\n",
    "# Rename columns for better understanding\n",
    "Y.columns = ['BM Prices']  # Dependent variable label\n",
    "X.columns = ['Past BM Prices', 'BM Volume', 'Wind Difference', 'Carbon Price', 'Gas Price',\n",
    "             'Past DAM Prices', 'Physical Notifications Volume', 'Net Interconnector Schedule', \n",
    "             'Renewable Forecast', 'Indep_Var_10', 'Demand Forecast', 'Future DAM Prices']  # Independent variable labels\n",
    "\n",
    "# Combine Y and X for correlation analysis\n",
    "combined_data = pd.concat([Y, X], axis=1)\n",
    "\n",
    "# Calculate the Spearman correlation matrix\n",
    "correlation_matrix = combined_data.corr(method='spearman')\n",
    "\n",
    "# Display the correlation matrix in the console\n",
    "print(\"Spearman Rank Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Create the heatmap plot\n",
    "plt.figure(figsize=(14, 12))  # Increase figure size for clarity\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1,\n",
    "            xticklabels=combined_data.columns, yticklabels=combined_data.columns, cbar_kws={\"shrink\": .75})  # Add color bar\n",
    "plt.title(\"Spearman Rank Correlation Heatmap: Balancing Market vs Forecast Variables\", fontsize=16, fontweight='bold', pad=20)  # Improved title\n",
    "plt.xticks(fontsize=12, rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.yticks(fontsize=12)  # Adjust font size of y-axis labels\n",
    "plt.xlabel('Variables', fontsize=14, labelpad=15)  # Custom x-axis label with padding\n",
    "plt.ylabel('Variables', fontsize=14, labelpad=15)  # Custom y-axis label with padding\n",
    "\n",
    "# Show the plot with tight layout\n",
    "plt.tight_layout()  # Ensure no overlap of labels or titles\n",
    "plt.savefig('/home/ciaran/Spearman_Correlation_BM.jpg', format='jpg', dpi=500)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7cc3f5",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8709845e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aade3d",
   "metadata": {},
   "source": [
    "Decomposition-\n",
    "\n",
    "Seasonal and Trend Decomposition using LOESS (STL Decomposition)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f63a5a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130cd18c",
   "metadata": {},
   "source": [
    "STL Decomposition for the DAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24fe539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# Load the data\n",
    "date_format = \"%d/%m/%Y %H:%M\"\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/DAM_VAR_1-3.csv\", index_col=\"DeliveryPeriod\", parse_dates=True, date_format=date_format)\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "# Handle missing or infinite values (replace infinite values, fill NaNs instead of dropping)\n",
    "dat.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "dat.fillna(method='ffill', inplace=True)  # Forward fill to handle NaNs\n",
    "\n",
    "# Select the time series for decomposition (e.g., DAM prices)\n",
    "Y = dat.iloc[:, 0]  # Assuming the first column contains the DAM prices\n",
    "\n",
    "# Apply STL decomposition (adjust the seasonal parameter based on data frequency)\n",
    "seasonal_period = 23  # e.g., if data is hourly and you expect daily seasonality, use 24\n",
    "stl = STL(Y, seasonal=seasonal_period)  # Adjust seasonal as necessary\n",
    "\n",
    "# Fit the decomposition model\n",
    "result = stl.fit()\n",
    "\n",
    "# Extract the components\n",
    "trend = result.trend\n",
    "seasonal = result.seasonal\n",
    "resid = result.resid\n",
    "\n",
    "# Improved plot settings for publication\n",
    "plt.figure(figsize=(12, 10), dpi=500)  # High DPI for better resolution\n",
    "\n",
    "# Define a consistent color palette\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Blue, orange, green, red\n",
    "\n",
    "# Plot the original time series\n",
    "plt.subplot(411)\n",
    "plt.plot(Y, label='Original', color=colors[0], linewidth=1.5)\n",
    "plt.title('Day-Ahead Market Prices', fontsize=14, weight='bold')\n",
    "plt.ylabel('Price (EUR)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "\n",
    "# Plot the trend component\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend', color=colors[1], linewidth=1.5)\n",
    "plt.title('Trend Component', fontsize=14, weight='bold')\n",
    "plt.ylabel('Trend', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "\n",
    "# Plot the seasonal component\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal, label='Seasonal', color=colors[2], linewidth=1.5)\n",
    "plt.title('Seasonal Component', fontsize=14, weight='bold')\n",
    "plt.ylabel('Seasonality', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "\n",
    "# Plot the residual component\n",
    "plt.subplot(414)\n",
    "plt.plot(resid, label='Residual', color=colors[3], linewidth=1.5)\n",
    "plt.title('Residual Component', fontsize=14, weight='bold')\n",
    "plt.ylabel('Residual', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "\n",
    "# Final layout adjustments\n",
    "plt.tight_layout(pad=2.0)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "\n",
    "# Save the figure as a high-quality PNG file\n",
    "plt.savefig('/home/ciaran/stl_decomposition_plot.jpg', dpi=500, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339c59fc",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcff9de",
   "metadata": {},
   "source": [
    "STL Decomposition for the BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# Load the data (adjust the date format and path accordingly)\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_format=date_format)\n",
    "\n",
    "# Drop unnecessary columns, fill missing values, and ensure only numeric data\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "# Select the time series for decomposition (assuming the first column contains the BM prices)\n",
    "Y = dat.iloc[:, 0]  # Adjust column index if necessary\n",
    "\n",
    "period = 48\n",
    "\n",
    "# Apply STL decomposition with the specified period\n",
    "stl = STL(Y, seasonal=13, period=period)\n",
    "\n",
    "# Fit the decomposition model\n",
    "result = stl.fit()\n",
    "\n",
    "# Extract the components\n",
    "trend = result.trend\n",
    "seasonal = result.seasonal\n",
    "resid = result.resid\n",
    "\n",
    "# Improved plot settings for publication\n",
    "plt.figure(figsize=(12, 10), dpi=500)  # High DPI for better resolution\n",
    "\n",
    "# Define a consistent color palette\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Blue, orange, green, red\n",
    "\n",
    "# Plot the original time series\n",
    "plt.subplot(411)\n",
    "plt.plot(Y, label='Original', color=colors[0], linewidth=1.5)\n",
    "plt.title('Balancing Market Prices', fontsize=14, weight='bold')\n",
    "plt.ylabel('Price (EUR)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "\n",
    "# Plot the trend component\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend', color=colors[1], linewidth=1.5)\n",
    "plt.title('Trend Component', fontsize=14, weight='bold')\n",
    "plt.ylabel('Trend', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "\n",
    "# Plot the seasonal component\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal, label='Seasonal', color=colors[2], linewidth=1.5)\n",
    "plt.title('Seasonal Component', fontsize=14, weight='bold')\n",
    "plt.ylabel('Seasonality', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "\n",
    "# Plot the residual component\n",
    "plt.subplot(414)\n",
    "plt.plot(resid, label='Residual', color=colors[3], linewidth=1.5)\n",
    "plt.title('Residual Component', fontsize=14, weight='bold')\n",
    "plt.ylabel('Residual', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "\n",
    "# Final layout adjustments\n",
    "plt.tight_layout(pad=2.0)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "\n",
    "# Save the figure as a high-quality PNG file\n",
    "plt.savefig('/home/ciaran/bm_stl_decomposition_plot.jpg', dpi=500, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5ca821",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd88dcf9",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f77fd52",
   "metadata": {},
   "source": [
    "Residual Analysis-\n",
    "\n",
    "Residual Plots\n",
    "\n",
    "Autocorrelation in Residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc6116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load your data\n",
    "dat1 = pd.read_csv(\"/home/ciaran/QRA&Q-Ave(QR&CP)/DAM_QRA/QR/LEAR_Q_DAM_1-12.csv\")\n",
    "\n",
    "# Extract real prices and predictions\n",
    "real_prices = dat1[['EURPrices+{}'.format(i) for i in range(0, 23)]].dropna().stack().reset_index(drop=True)\n",
    "predictions = dat1[['EURPrices+{}_Forecast_50'.format(i) for i in range(0, 23)]].dropna().stack().reset_index(drop=True)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = real_prices - predictions\n",
    "# Create a DataFrame for easier plotting\n",
    "df_residuals = pd.DataFrame({\n",
    "    'Fitted Values': predictions,\n",
    "    'Residuals': residuals\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Highlight outliers\n",
    "std_residuals = df_residuals['Residuals'].std()\n",
    "outliers = abs(df_residuals['Residuals']) > 2 * std_residuals\n",
    "\n",
    "sns.scatterplot(x='Fitted Values', y='Residuals', data=df_residuals, hue=outliers, \n",
    "                palette={False: 'blue', True: 'orange'}, alpha=0.6)\n",
    "\n",
    "# Add LOWESS smoothing line\n",
    "sns.regplot(x='Fitted Values', y='Residuals', data=df_residuals, lowess=True, \n",
    "            line_kws={'color': 'red', 'lw': 1}, scatter=False)\n",
    "\n",
    "plt.axhline(0, color='black', lw=1, ls='--')\n",
    "plt.ylim(-100, 100)  # Adjust y-axis limits\n",
    "plt.title('Residual Plot of Predicted vs Actual Prices (DAM/BM)')\n",
    "plt.xlabel('Fitted Values (Predicted Prices in EUR)')\n",
    "plt.ylabel('Residuals (Actual - Predicted Prices in EUR)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97670b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1 = pd.read_csv(\"/home/ciaran/QRA&Q-Ave(QR&CP)/DAM_QRA/QR/lgbm_Q_DAM_1-12.csv\")\n",
    "\n",
    "# Extract real prices and predictions\n",
    "real_prices = dat1[['EURPrices+{}'.format(i) for i in range(0, 23)]].dropna().stack().reset_index(drop=True)\n",
    "predictions = dat1[['EURPrices+{}_Forecast_50'.format(i) for i in range(0, 23)]].dropna().stack().reset_index(drop=True)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = real_prices - predictions\n",
    "# Create a DataFrame for easier plotting\n",
    "df_residuals = pd.DataFrame({\n",
    "    'Fitted Values': predictions,\n",
    "    'Residuals': residuals\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Highlight outliers\n",
    "std_residuals = df_residuals['Residuals'].std()\n",
    "outliers = abs(df_residuals['Residuals']) > 2 * std_residuals\n",
    "\n",
    "sns.scatterplot(x='Fitted Values', y='Residuals', data=df_residuals, hue=outliers, \n",
    "                palette={False: 'blue', True: 'orange'}, alpha=0.6)\n",
    "\n",
    "# Add LOWESS smoothing line\n",
    "sns.regplot(x='Fitted Values', y='Residuals', data=df_residuals, lowess=True, \n",
    "            line_kws={'color': 'red', 'lw': 1}, scatter=False)\n",
    "\n",
    "plt.axhline(0, color='black', lw=1, ls='--')\n",
    "plt.ylim(-100, 100)  # Adjust y-axis limits\n",
    "plt.title('Residual Plot of Predicted vs Actual Prices (DAM/BM)')\n",
    "plt.xlabel('Fitted Values (Predicted Prices in EUR)')\n",
    "plt.ylabel('Residuals (Actual - Predicted Prices in EUR)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3816aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1 = pd.read_csv(\"/home/ciaran/QRA&Q-Ave(QR&CP)/DAM_QRA/QR/rf_Q_DAM_1-12.csv\")\n",
    "\n",
    "# Extract real prices and predictions\n",
    "real_prices = dat1[['EURPrices+{}'.format(i) for i in range(0, 23)]].dropna().stack().reset_index(drop=True)\n",
    "predictions = dat1[['EURPrices+{}_Forecast_50'.format(i) for i in range(0, 23)]].dropna().stack().reset_index(drop=True)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = real_prices - predictions\n",
    "# Create a DataFrame for easier plotting\n",
    "df_residuals = pd.DataFrame({\n",
    "    'Fitted Values': predictions,\n",
    "    'Residuals': residuals\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Highlight outliers\n",
    "std_residuals = df_residuals['Residuals'].std()\n",
    "outliers = abs(df_residuals['Residuals']) > 2 * std_residuals\n",
    "\n",
    "sns.scatterplot(x='Fitted Values', y='Residuals', data=df_residuals, hue=outliers, \n",
    "                palette={False: 'blue', True: 'orange'}, alpha=0.6)\n",
    "\n",
    "# Add LOWESS smoothing line\n",
    "sns.regplot(x='Fitted Values', y='Residuals', data=df_residuals, lowess=True, \n",
    "            line_kws={'color': 'red', 'lw': 1}, scatter=False)\n",
    "\n",
    "plt.axhline(0, color='black', lw=1, ls='--')\n",
    "plt.ylim(-100, 100)  # Adjust y-axis limits\n",
    "plt.title('Residual Plot of Predicted vs Actual Prices (DAM/BM)')\n",
    "plt.xlabel('Fitted Values (Predicted Prices in EUR)')\n",
    "plt.ylabel('Residuals (Actual - Predicted Prices in EUR)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cf237a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a08aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load your data\n",
    "dat1 = pd.read_csv(\"/home/ciaran/QRA&Q-Ave(QR&CP)/BM_QRA/QR/LEAR_Q_1-12.csv\")\n",
    "dat1 = pd.DataFrame(dat1)\n",
    "\n",
    "# Extract real prices and predictions for the BM market\n",
    "real_prices_BM = dat1[['lag_{}y'.format(i) for i in range(2, 18)]].dropna().stack().reset_index(drop=True)\n",
    "predictions_BM = dat1[['lag_{}y_Forecast_50'.format(i) for i in range(2, 18)]].dropna().stack().reset_index(drop=True)\n",
    "\n",
    "# Calculate residuals for the BM market\n",
    "residuals_BM = real_prices_BM - predictions_BM\n",
    "\n",
    "# Create a DataFrame for easier plotting\n",
    "df_residuals_BM = pd.DataFrame({\n",
    "    'Fitted Values': predictions_BM,\n",
    "    'Residuals': residuals_BM\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Highlight outliers\n",
    "std_residuals_BM = df_residuals_BM['Residuals'].std()\n",
    "outliers_BM = abs(df_residuals_BM['Residuals']) > 2 * std_residuals_BM\n",
    "\n",
    "sns.scatterplot(x='Fitted Values', y='Residuals', data=df_residuals_BM, hue=outliers_BM, \n",
    "                palette={False: 'blue', True: 'orange'}, alpha=0.6)\n",
    "\n",
    "# Add LOWESS smoothing line\n",
    "sns.regplot(x='Fitted Values', y='Residuals', data=df_residuals_BM, lowess=True, \n",
    "            line_kws={'color': 'red', 'lw': 1}, scatter=False)\n",
    "\n",
    "plt.axhline(0, color='black', lw=1, ls='--')\n",
    "plt.ylim(-500, 500)  # Adjust y-axis limits as necessary\n",
    "plt.title('Residual Plot of Predicted vs Actual Prices (BM)')\n",
    "plt.xlabel('Fitted Values (Predicted Prices in EUR)')\n",
    "plt.ylabel('Residuals (Actual - Predicted Prices in EUR)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e51d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1 = pd.read_csv(\"/home/ciaran/QRA&Q-Ave(QR&CP)/BM_QRA/QR/lgbm_Q_1-12.csv\")\n",
    "dat1 = pd.DataFrame(dat1)\n",
    "\n",
    "# Extract real prices and predictions for the BM market\n",
    "real_prices_BM = dat1[['lag_{}y'.format(i) for i in range(2, 18)]].dropna().stack().reset_index(drop=True)\n",
    "predictions_BM = dat1[['lag_{}y_Forecast_50'.format(i) for i in range(2, 18)]].dropna().stack().reset_index(drop=True)\n",
    "\n",
    "# Calculate residuals for the BM market\n",
    "residuals_BM = real_prices_BM - predictions_BM\n",
    "\n",
    "# Create a DataFrame for easier plotting\n",
    "df_residuals_BM = pd.DataFrame({\n",
    "    'Fitted Values': predictions_BM,\n",
    "    'Residuals': residuals_BM\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Highlight outliers\n",
    "std_residuals_BM = df_residuals_BM['Residuals'].std()\n",
    "outliers_BM = abs(df_residuals_BM['Residuals']) > 2 * std_residuals_BM\n",
    "\n",
    "sns.scatterplot(x='Fitted Values', y='Residuals', data=df_residuals_BM, hue=outliers_BM, \n",
    "                palette={False: 'blue', True: 'orange'}, alpha=0.6)\n",
    "\n",
    "# Add LOWESS smoothing line\n",
    "sns.regplot(x='Fitted Values', y='Residuals', data=df_residuals_BM, lowess=True, \n",
    "            line_kws={'color': 'red', 'lw': 1}, scatter=False)\n",
    "\n",
    "plt.axhline(0, color='black', lw=1, ls='--')\n",
    "plt.ylim(-500, 500)  # Adjust y-axis limits as necessary\n",
    "plt.title('Residual Plot of Predicted vs Actual Prices (BM)')\n",
    "plt.xlabel('Fitted Values (Predicted Prices in EUR)')\n",
    "plt.ylabel('Residuals (Actual - Predicted Prices in EUR)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df41bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1 = pd.read_csv(\"/home/ciaran/QRA&Q-Ave(QR&CP)/BM_QRA/QR/rf_Q_1-12.csv\")\n",
    "dat1=pd.DataFrame(dat1)\n",
    "\n",
    "# Extract real prices and predictions for the BM market\n",
    "real_prices_BM = dat1[['lag_{}y'.format(i) for i in range(2, 18)]].dropna().stack().reset_index(drop=True)\n",
    "predictions_BM = dat1[['lag_{}y_Forecast_50'.format(i) for i in range(2, 18)]].dropna().stack().reset_index(drop=True)\n",
    "\n",
    "# Calculate residuals for the BM market\n",
    "residuals_BM = real_prices_BM - predictions_BM\n",
    "\n",
    "# Create a DataFrame for easier plotting\n",
    "df_residuals_BM = pd.DataFrame({\n",
    "    'Fitted Values': predictions_BM,\n",
    "    'Residuals': residuals_BM\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Highlight outliers\n",
    "std_residuals_BM = df_residuals_BM['Residuals'].std()\n",
    "outliers_BM = abs(df_residuals_BM['Residuals']) > 2 * std_residuals_BM\n",
    "\n",
    "sns.scatterplot(x='Fitted Values', y='Residuals', data=df_residuals_BM, hue=outliers_BM, \n",
    "                palette={False: 'blue', True: 'orange'}, alpha=0.6)\n",
    "\n",
    "# Add LOWESS smoothing line\n",
    "sns.regplot(x='Fitted Values', y='Residuals', data=df_residuals_BM, lowess=True, \n",
    "            line_kws={'color': 'red', 'lw': 1}, scatter=False)\n",
    "\n",
    "plt.axhline(0, color='black', lw=1, ls='--')\n",
    "plt.ylim(-500, 500)  # Adjust y-axis limits as necessary\n",
    "plt.title('Residual Plot of Predicted vs Actual Prices (BM)')\n",
    "plt.xlabel('Fitted Values (Predicted Prices in EUR)')\n",
    "plt.ylabel('Residuals (Actual - Predicted Prices in EUR)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe690821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a259b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4138b65",
   "metadata": {},
   "source": [
    "Autocorrelation in Residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c811691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Load your data\n",
    "dat1 = pd.read_csv(\"/home/ciaran/QRA&Q-Ave(QR&CP)/DAM_QRA/QR/LEAR_Q_DAM_1-12.csv\")\n",
    "\n",
    "# Extract real prices and predictions\n",
    "real_prices = dat1[['EURPrices+{}'.format(i) for i in range(0, 23)]].dropna().stack().reset_index(drop=True)\n",
    "predictions = dat1[['EURPrices+{}_Forecast_50'.format(i) for i in range(0, 23)]].dropna().stack().reset_index(drop=True)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = real_prices - predictions\n",
    "\n",
    "# Plot the Autocorrelation Function (ACF) of the residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_acf(residuals, lags=40, alpha=0.05)\n",
    "plt.title('Autocorrelation Function (ACF) of Residuals (DAM)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Perform the Ljung-Box test for autocorrelation at multiple lags\n",
    "ljung_box_results = acorr_ljungbox(residuals, lags=[10, 20, 30, 40], return_df=True)\n",
    "\n",
    "# Display the Ljung-Box test results\n",
    "print(\"Ljung-Box Test Results:\")\n",
    "print(ljung_box_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf5b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1 = pd.read_csv(\"/home/ciaran/QRA&Q-Ave(QR&CP)/DAM_QRA/QR/lgbm_Q_DAM_1-12.csv\")\n",
    "\n",
    "\n",
    "# Extract real prices and predictions\n",
    "real_prices = dat1[['EURPrices+{}'.format(i) for i in range(0, 23)]].dropna().stack().reset_index(drop=True)\n",
    "predictions = dat1[['EURPrices+{}_Forecast_50'.format(i) for i in range(0, 23)]].dropna().stack().reset_index(drop=True)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = real_prices - predictions\n",
    "\n",
    "# Plot the Autocorrelation Function (ACF) of the residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_acf(residuals, lags=40, alpha=0.05)\n",
    "plt.title('Autocorrelation Function (ACF) of Residuals (DAM)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Perform the Ljung-Box test for autocorrelation at multiple lags\n",
    "ljung_box_results = acorr_ljungbox(residuals, lags=[10, 20, 30, 40], return_df=True)\n",
    "\n",
    "# Display the Ljung-Box test results\n",
    "print(\"Ljung-Box Test Results:\")\n",
    "print(ljung_box_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1 = pd.read_csv(\"/home/ciaran/QRA&Q-Ave(QR&CP)/DAM_QRA/QR/rf_Q_DAM_1-12.csv\")\n",
    "\n",
    "# Extract real prices and predictions\n",
    "real_prices = dat1[['EURPrices+{}'.format(i) for i in range(0, 23)]].dropna().stack().reset_index(drop=True)\n",
    "predictions = dat1[['EURPrices+{}_Forecast_50'.format(i) for i in range(0, 23)]].dropna().stack().reset_index(drop=True)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = real_prices - predictions\n",
    "\n",
    "# Plot the Autocorrelation Function (ACF) of the residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_acf(residuals, lags=40, alpha=0.05)\n",
    "plt.title('Autocorrelation Function (ACF) of Residuals (DAM)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Perform the Ljung-Box test for autocorrelation at multiple lags\n",
    "ljung_box_results = acorr_ljungbox(residuals, lags=[10, 20, 30, 40], return_df=True)\n",
    "\n",
    "# Display the Ljung-Box test results\n",
    "print(\"Ljung-Box Test Results:\")\n",
    "print(ljung_box_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb71c7a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeba32a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Load your data\n",
    "dat1 = pd.read_csv(\"/home/ciaran/QRA&Q-Ave(QR&CP)/BM_QRA/QR/LEAR_Q_1-12.csv\")\n",
    "dat1 = pd.DataFrame(dat1)\n",
    "\n",
    "\n",
    "# Extract real prices and predictions for the BM market\n",
    "real_prices_BM = dat1[['lag_{}y'.format(i) for i in range(2, 18)]].dropna().stack().reset_index(drop=True)\n",
    "predictions_BM = dat1[['lag_{}y_Forecast_50'.format(i) for i in range(2, 18)]].dropna().stack().reset_index(drop=True)\n",
    "\n",
    "# Calculate residuals for the BM market\n",
    "residuals = real_prices_BM - predictions_BM\n",
    "\n",
    "# Plot the Autocorrelation Function (ACF) of the residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_acf(residuals, lags=40, alpha=0.05)\n",
    "plt.title('Autocorrelation Function (ACF) of Residuals (BM)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Perform the Ljung-Box test for autocorrelation at multiple lags\n",
    "ljung_box_results = acorr_ljungbox(residuals, lags=[10, 20, 30, 40], return_df=True)\n",
    "\n",
    "# Display the Ljung-Box test results\n",
    "print(\"Ljung-Box Test Results:\")\n",
    "print(ljung_box_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1 = pd.read_csv(\"/home/ciaran/QRA&Q-Ave(QR&CP)/BM_QRA/QR/lgbm_Q_1-12.csv\")\n",
    "dat1 = pd.DataFrame(dat1)\n",
    "\n",
    "# Extract real prices and predictions for the BM market\n",
    "real_prices_BM = dat1[['lag_{}y'.format(i) for i in range(2, 18)]].dropna().stack().reset_index(drop=True)\n",
    "predictions_BM = dat1[['lag_{}y_Forecast_50'.format(i) for i in range(2, 18)]].dropna().stack().reset_index(drop=True)\n",
    "\n",
    "# Calculate residuals for the BM market\n",
    "residuals = real_prices_BM - predictions_BM\n",
    "\n",
    "# Plot the Autocorrelation Function (ACF) of the residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_acf(residuals, lags=40, alpha=0.05)\n",
    "plt.title('Autocorrelation Function (ACF) of Residuals (BM)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Perform the Ljung-Box test for autocorrelation at multiple lags\n",
    "ljung_box_results = acorr_ljungbox(residuals, lags=[10, 20, 30, 40], return_df=True)\n",
    "\n",
    "# Display the Ljung-Box test results\n",
    "print(\"Ljung-Box Test Results:\")\n",
    "print(ljung_box_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bcaa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1 = pd.read_csv(\"/home/ciaran/QRA&Q-Ave(QR&CP)/BM_QRA/QR/rf_Q_1-12.csv\")\n",
    "dat1=pd.DataFrame(dat1)\n",
    "\n",
    "\n",
    "# Extract real prices and predictions for the BM market\n",
    "real_prices_BM = dat1[['lag_{}y'.format(i) for i in range(2, 18)]].dropna().stack().reset_index(drop=True)\n",
    "predictions_BM = dat1[['lag_{}y_Forecast_50'.format(i) for i in range(2, 18)]].dropna().stack().reset_index(drop=True)\n",
    "\n",
    "# Calculate residuals for the BM market\n",
    "residuals = real_prices_BM - predictions_BM\n",
    "\n",
    "# Plot the Autocorrelation Function (ACF) of the residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_acf(residuals, lags=40, alpha=0.05)\n",
    "plt.title('Autocorrelation Function (ACF) of Residuals (BM)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Perform the Ljung-Box test for autocorrelation at multiple lags\n",
    "ljung_box_results = acorr_ljungbox(residuals, lags=[10, 20, 30, 40], return_df=True)\n",
    "\n",
    "# Display the Ljung-Box test results\n",
    "print(\"Ljung-Box Test Results:\")\n",
    "print(ljung_box_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4df8c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2e313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df1f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
